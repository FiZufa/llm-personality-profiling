{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11507cb2",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge (Qwen2.5-7B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108d7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import requests  # Assuming API access via HTTP\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed34faf",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a24ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_json('result/llama_N32.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_response(text, actual_type, reasoning):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Please evaluate this MBTI prediction response by providing:\n",
    "\n",
    "    1. Scores (1-5 scale) for:\n",
    "        - Trait Consistency: /5 (alignment with MBTI theory)\n",
    "        - Evidence Quality: /5 (psychological validity of examples)\n",
    "        - Logical Flow: /5 (coherence of reasoning)\n",
    "   \n",
    "    2. Type Accuracy: 1 if correct, 0 if incorrect\n",
    "\n",
    "    3. Detailed feedback explaining your assessment\n",
    "\n",
    "    === PLEASE USE THIS EXACT FORMAT ===\n",
    "    SCORES:\n",
    "    Trait Consistency: X/5\n",
    "    Evidence Quality: X/5\n",
    "    Logical Flow: X/5\n",
    "    Type Accuracy: X\n",
    "\n",
    "    FEEDBACK:\n",
    "    [Your detailed feedback here]\n",
    "\n",
    "    Text: {text}\n",
    "    Actual type: {actual_type}\n",
    "    Reasoning: {reasoning}\n",
    "\n",
    "    Response: \n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI(\n",
    "        # If the environment variable is not configured, replace the following line with your API key: api_key=\"sk-xxx\",\n",
    "        api_key=\"***\",\n",
    "        base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen2.5-14b-instruct\",  # Model list: https://www.alibabacloud.com/help/en/model-studio/getting-started/models\n",
    "        messages=[\n",
    "            # {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512d601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_function(df_name):\n",
    "    print('Judging Deepseek N0')\n",
    "\n",
    "    results = []\n",
    "    for idx, row in df_name.iterrows():\n",
    "        text = row['text:']\n",
    "        actual_type = row['actual_type']\n",
    "        response_msg = row['response']\n",
    "\n",
    "        response = judge_response(text, actual_type, response_msg)\n",
    "\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"mbti_actual\": actual_type,\n",
    "            \"reasoning\": response_msg,\n",
    "            \"judgment\": response\n",
    "        })\n",
    "\n",
    "        print(f\"Processed row {idx + 1}/{len(df_name)}\")\n",
    "\n",
    "\n",
    "    with open('judge_Deepseek_N0.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(\"Judgement result is saved to 'judge_Deepseek_N0.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Judging llama_N32')\n",
    "\n",
    "results = []\n",
    "for idx, row in result_df.iterrows():\n",
    "    text = row['text']\n",
    "    actual_type = row['actual_type']\n",
    "    response_msg = row['response']\n",
    "\n",
    "    response = judge_response(text, actual_type, response_msg)\n",
    "\n",
    "    results.append({\n",
    "        \"text\": text,\n",
    "        \"mbti_actual\": actual_type,\n",
    "        \"reasoning\": response_msg,\n",
    "        \"judgment\": response\n",
    "    })\n",
    "\n",
    "    print(f\"Processed row {idx + 1}/{len(result_df)}\")\n",
    "\n",
    "\n",
    "with open('judge_llama_N32.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Judgement result is saved to 'judge_llama_N32.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidovax-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
